# 13 Elasticsearch的优化建议

## 13.1 硬件选择

Elasticsearch是基于Lucene的，所有的索引和文档数据都是存储在本地的磁盘中，具体路径可在ES配置文件`config/elasticsearch.yml`中配置，如下：
```yaml
# 数据文件
path.data: ./data
# 日志文件
path.logs: ./logs
```

磁盘在现代服务器上通常都是瓶颈。Elasticsearch重度使用磁盘，**磁盘的吞吐量越大，节点就越稳定。**

> 优化磁盘I/O的技巧

* 使用SSD（固态硬盘）
* 使用RAID 0（一种硬盘阵列）：读写速度快，能够提高磁盘容量。但是当一快磁盘故障整个就故障了。不要使用镜像或者奇偶校验RAID，因为副本已经提供该功能。
* 使用多块硬盘，并允许Elasticsearch通过多个`path.data`目录配置把数据条带化分配到它们上面。
* 不要使用远程挂载的存储，如NFS或者SMB/CIFS


## 13.2 分片策略

### 13.2.1 合理设置分片数

> 分片的代价

* 一个分片的底层即为一个Lucene索引，会消耗一定文件句柄、内存以及CPU运转
* 每一个搜索请求都需要命中索引中的一个分片。如果多个分片处于同一个节点，这些分片就会竞争使用相同的资源
* 用于计算相关度的词项统计信息是基于分片的。如果分片过多，就会导致很低的相关度

> 分片遵循的原则

* 控制每个分片占用的磁盘容量不超过ES的最大JVN堆栈空间设置（JVM一般不超过32M，参考下面JVM设置原则）。
    
    `栗子` 假设索引的总容量在500G左右，那么分片大小在16个左右即可。当然，最好同时考虑原则2

* 分片数一般不要超过节点数的3倍。
    
    `说明` 一般来书一个节点就是一台物理机。如果分片数打打超过节点数，就会造成一个节点上多个分片，处理在索引时会竞争资源外，一旦该节点故障，即使保持了1个以上的副本，同样有可能导致数据丢失，集群无法恢复。
   
* 主分片、副本、节点最大数的计算公式如下：
    
    `公式` 节点数 <= 主分片数 * (副本数 + 1)

### 13.2.2 推迟分片分配

对于节点瞬时中断的问题，默认情况下，集群会等待**1min**来查看节点是否会重新加入。
如果这个节点在此瞬间重新加入，重新加入的节点会保持现有的分片数据，不会触发新的分片分配。
这样就可以减少Elasticsearch在自动再平衡可用分片时所带来的的极大开销。

> 延长再均衡时间

通过修改参数`delayed_time`，延长再均衡时间。可以全局设置，也可以在索引级别修改

```text
PUT /_all/_settings 
{
  "settings": {
    "index.unassigned.node_left.delayed_timeout": "5m" 
  }
}
```

## 13.3 路由选择

当进行文档查询的时候，Elasticsearch是通过如下公式计算得到文档存放的分片：

`shard = hash(routing) % number_of_primary_shards`
其中，routing默认值是文档id，也可以采用自定义值。

* **不带routing查询**
   
   在查询时，因为不知道要查询的数据具体在哪个分片上，所以整个过程分为2个步骤
   * 分发： 请求到达协调节点后，协调节点将查询请求分发到每个分片上
   * 聚合： 协调节点搜集到每个分片上的查询结果，将查询结果进行排序，然后将结果返回给用户

* **带routing查询**

    查询时，可以根据routing信息定位到某个分片查询，不需要查询所有分片。
    
## 13.4 写入速度优化

ES默认配置，是综合了数据可靠性、写入速度、搜索实时性等因素。
实际使用时，可以按照实际情况，进行偏向性的优化。

> 写入速度优化概述

针对搜索性能要求不高，但对写入要求较高的场景，我们需要尽可能的选择恰当的写优化策略。
综合来说，可以考虑一下几个方面来提升索引的性能：
* 在集群正常运行的前提下，如果是集群首次批量导入数据时，可以将副本数设置为0，导入完毕后再将副本数调整为正常值，这样副分片就只需要复制，节约了构建索引的时间
* 增大Translog flush间隔，目的是降低Iops（即每秒的输入输出量(或读写次数)，是衡量磁盘性能的主要指标之一）、WriteBlock
* 增大Index Refresh间隔，以减少I/O，更重要的是减少Segment Merge（段合并）的次数
* 善用Bulk请求，调整Bulk线程池和队列
* 优化磁盘的任务均匀情况，将shard尽量均匀分布到物理主机的各个磁盘
* 优化节点见的任务分布，将任务尽量均匀的发到各个节点
* 优化Lucene层的索引建立，以降低CPU和IO，如禁用`_all`字段

> 重要概念

* `flush`：是指触发lucene commit，也就是将缓存中的数据写入到磁盘，并清空translog日志文件
* `refresh`: 是指从内存到文件系统缓存的过程。此时该文档就可以被搜索到，但是该文档还没有存储到磁盘上，如果机器宕机了，数据就会丢失

### 13.4.1 增大Translog flush间隔

默认情况下，translog的持久化策略为每个请求都flush。对应配置如下：
```yaml
index.translog.durability: request 
```

这是写入速度的最大因素，但是也只有这样，写操作才有可能是可靠的。

**如果系统能够接受一定概率的数据的丢失**（如，数据写入主分片成功，尚未复制到副本时，主机断电。由于数据既没有刷到Lucene，translog也没有刷入磁盘，恢复时，translog中没有这个数据，导致数据丢失），
则可以调整translog持久化策略为周期性和一定大小的时候flush。如：
```yaml
# async表示translog的刷盘策略按sync_interval配置指定的时间周期进行
index.translog.durability: async
# 加大translog刷盘间隔时间。默认为5s，不可低于10ms
index.translog.sync_interval: 120s
# （段合并的时候设置）设置当超过一定大小时进行flush。当超过这个大小会导致refresh操作，产生新的Lucene分段。默认值为 512MB
index.translog.flush_threshold_size: 1024mb
```

### 13.4.2 增大Index Refresh间隔

默认情况下索引的refresh_interval为1s，这意味着数据每隔1s就会从es缓存中写入文件系统缓存中，以保证数据可以被搜索到。
每次索引的refresh会产生一个新的Lucene段（即segment），segment在符合一定条件后，会自动合并，因此这会导致频繁的segment merge行为。
如果不需要这么高的搜索实时性，应该降低索引refresh周期。

```yaml
index.refresh_interval: 120s
```

### 13.4.3 段合并优化

* 降低段产生的数量

    * 增大Index Refresh间隔（见13.4.2小节）
    * 增大分片的indexing buffer
        * indexing buffer在为doc建立索引时使用，当缓冲满时会刷入磁盘， 生成一个新的segment，这是除refresh_interval刷新索引外，另一个生成新segment的机会。
        * 每个shard有自己的indexing buffer， 配置修改如下
            ```yaml
            indices.memory.index_buffer_size: 15% # 默认为10%，可以适当增大该值 
            ```
    * 尽量减少文档的更新操作

* 降低最大段的大小，避免较大的段继续参与merge，节省系统资源，但是最终会有多个段
    * 增大`index.merge.policy.segments_per_tier`，默认为10，值越小表示需要越多的merge。
        
        另：该值需要大于等于`index.merge.policy.max_merge_at_once`(表示默认一次最多归并segment的个数，默认为10)
    
    * 减小`index.merge.policy.max_merged_segment`，默认5GB。当段的大小超过此值后，将不再参与merge操作

* 当Index不再有写入操作时，对其进行force merge，最好force merge成1个段。
    如此可以提升查询速度，减少内存开销，但是force merge时会消耗大量的I/O、CUP资源
   
### 13.4.4 善用Bulk请求

* 当需要执行批量写操作时，Bulk请求比一个索引请求只写单个文档效率高得多，但是Bulk请求的整体字节数最好避免超过几十兆字节，否则会给内存带来极大压力。
* Bulk线程池和队列的优化：建立索引的过程属于**计算密集型任务**，应使用固定大小的线程池，来不及处理的任务放到队列中。
    * 线程池数：CPU核数 + 1，这也是默认配置，可以避免过多的上下文切换
    * 队列数：队列数可以适当增加，但是一定要严格控制大小，过大的队列导致较高的GC压力，并可能导致FGC频繁发生
    * 要注意bulk线程池队列的reject情况，出现reject代表ES的bulk队列已满，客户端收到429错误(TOO_MANY_REQUESTS)。
    不可忽略这个异常，否则写入系统的数据会少于预期。即使客户端正确处理了429错误，我们仍然应该尽量避免产生reject。

### 13.4.5 单节点磁盘间的任务均衡

* 首先在配置文件conf/Elasticsearch.yml中为path.data配置多个路径来使用多块磁盘，多磁盘带来的并行写的优势可以增加吞吐量
* 多磁盘写入可能会带来任务不均衡的问题，Elasticsearch在分配shard时，落到各磁盘上的shard可能并不均匀。
    这种不均匀可能会导致某些磁盘繁忙，利用率在较长时间内持续达到100%，而某些磁盘可能使用率很低甚至为0，这种不均匀达到一定程度会对写入性能产生负面影响。
  
  对于此种场景，有两种策略可以考虑：
    * 简单轮询：在系统初始阶段，简单轮询的效果是最均匀的。
    * 基于可用空间的动态加权轮询：以可用空间作为权重，在磁盘之间加权轮询
    
### 13.4.6 节点间的任务均衡

为了节点间的任务尽量均衡，数据写入客户端应该把bulk请求轮询发送到各个节点。此时可以 **考虑使用Java API或REST API的bulk接口发送数据**。
 
当使用Java API或REST API的bulk接口发送数据时，客户端将会轮询发送到集群节点，节点列表取决于：
* 使用Java API时，当设置client.transport.sniff为true（默认为false） 时，列表为所有数据节点，否则节点列表为构建客户端对象时传入的节点列表。
* 使用REST API时，列表为构建对象时添加进去的节点。 

    在此 **建议使用REST API** ,Java API会在未来的版本中废弃，REST API有良好的版本兼容性好。
    理论上，Java API在序列化上有性能优势，但是只有在吞吐量非常大时才值得考虑序列化的开销带来的影响， 通常搜索并不是高吞吐量的业务。

观察 `bulk` 请求在不同节点上的处理情况：

```text
GET _cat/thread_pool
```

### 13.4.7 优化索引的建立

* 自动生成docID，避免es对自定义ID验证操作

* 调整字段Mapping
    * 减少不必要的字段数量
    * 将不需要创建索引字段的index属性设置为not_analyzed，对字段不分词或者不建立索引，减少相应的操作，特别是binary类型
    * 减少字段内容长度
    * 使用不同的分析器（analyzer），不同分析器之间的运算复杂度也不相同
    
* 调整_source字段
    
    _source字段用于存储doc原始数据，对于部分不需要存储的字段，可以使用`includes excludes`过滤，或者禁用_source，但是一般实际场景不会禁用。
    
* 禁用_all

    _all中包含所有字段分词后的关键词，作用是可以搜索的时候不指定特定的字段，从所有字段中检索。
    从ES6.0之后，_all字段默认不启用。
 
* 对不需要评分的字段禁用Norms
    
    Norms字段用于在搜索时计算doc的评分，如果不需要评分，可以将其禁用：
    ```text
     "title": {
          "type": "string",
          "norms": {
              "enabled": false
          }
      }
    ```